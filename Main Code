import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load data
data = pd.read_excel('SET DATA.xlsx')

# Convert 'AQI Index' to numeric and set Date as index
data['AQI Index'] = pd.to_numeric(data['AQI Index'], errors='coerce')
data.set_index('Date', inplace=True)

# Check missing values
missing_values = data.isnull().sum()
missing_percentage = (missing_values / len(data)) * 100

# Drop columns with >30% missing values
columns_to_drop = ['PM10 (µg/m³)', 'NH3 (µg/m³)']
data = data.drop(columns=columns_to_drop)

# Time-series friendly imputation (forward-fill)
data = data.ffill().bfill()  # Fill forward then backward

# Verify no remaining missing values
print(data.isnull().sum())

# Sort index for time-series operations
data = data.sort_index()

# Save cleaned data
data.to_excel('AQI.xlsx')

# Visualization
plt.figure(figsize=(14, 7))
data['AQI Index'].plot(title='AQI Index Trend Over Time', color='darkgreen')
plt.xlabel('Date')
plt.ylabel('AQI Value')
plt.grid(True)
plt.tight_layout()
plt.show()

# Additional analysis (example correlation matrix)
numeric_cols = data.select_dtypes(include=np.number).columns
corr_matrix = data[numeric_cols].corr()

plt.figure(figsize=(12, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=90)
plt.yticks(range(len(corr_matrix)), corr_matrix.columns)
plt.title('Feature Correlation Matrix')
plt.tight_layout()
plt.show()

series = data['AQI Index']
series

print(series.isnull().sum())

# Rolling average for trend
rolling_mean = series.rolling(window=12).mean()

plt.figure(figsize=(8,4))
plt.plot(series, label='Original Series')
plt.plot(rolling_mean, color='red', label='Trend (Rolling Mean)')
plt.legend()
plt.show()

data= data.copy()

data['year'] = data.index.year
data['month'] = data.index.month_name()

month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

for month in month_order:
    monthly_data = data[data['month'] == month]
    
    monthly_avg = monthly_data.groupby('year')['AQI Index'].mean()
    
    # Plot the data
    plt.figure(figsize=(5, 3))
    monthly_avg.plot(kind='line', marker='o', color='b')
    plt.title(f'{month} AQI Across Years')
    plt.xlabel('Year')
    plt.ylabel('Average AQI')
    plt.tight_layout()
    plt.show()

mean_aqi_by_month = data.groupby('month')['AQI Index'].mean()

month_order = ['January', 'February', 'March', 'April', 'May', 'June', 
               'July', 'August', 'September', 'October', 'November', 'December']
mean_aqi_by_month = mean_aqi_by_month.reindex(month_order)

month_to_month_change = mean_aqi_by_month.diff()

colors = ['green' if x < 0 else 'red' for x in month_to_month_change]

month_to_month_change.plot(kind='bar', color=colors, figsize=(8, 4))

plt.title('Month-to-Month AQI Change')
plt.xlabel('Month')
plt.ylabel('AQI Change')
plt.axhline(0, color='black', linestyle='--', linewidth=0.8)  # Line at y=0 for reference
plt.xticks(rotation=45)
plt.tight_layout()

plt.show()

from statsmodels.tsa.stattools import adfuller

# Perform ADF test
result = adfuller(series)
print('ADF Statistic:', result[0])
print('\np-value:', result[1])
if result[1] < 0.05:
    print("\nThe series is stationary.")
else:
    print("\nThe series is not stationary.")

from pandas.plotting import autocorrelation_plot

autocorrelation_plot(series)
plt.show()

from hurst import compute_Hc

H, c, data = compute_Hc(series, kind='change', simplified=True)
print(f"Hurst Exponent: {H}")

if H < 0.5:
    print("\nThe series shows mean-reverting behavior.")
elif H == 0.5:
    print("\nThe series is a random walk.")
else:
    print("\nThe series shows long-term dependence.")

from scipy.stats import linregress

x = range(len(series))
slope, intercept, r_value, p_value, std_err = linregress(x, series)
line = slope * x + intercept

plt.scatter(x, series, label='Data')
plt.plot(x, line, color='red', label='Linear Fit')
plt.legend()
plt.show()

print(f"R-squared: {r_value**2}")

from statsmodels.tsa.seasonal import seasonal_decompose

# Decompose the time series
result = seasonal_decompose(series, model='additive', period=365)
result.plot()
plt.show()

from scipy.stats import skew, kurtosis
print(skew(series))

print(kurtosis(series))

import statsmodels.api as sm
import pandas as pd
from scipy.stats import chi2
from statsmodels.stats.diagnostic import het_arch, linear_reset
from statsmodels.tsa.stattools import adfuller
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from sklearn.preprocessing import StandardScaler


data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

# Create a simple linear model (used in all tests)
data['Time'] = (data.index - data.index[0]).days  # Days since the first date
X = sm.add_constant(data['Time'])
y = data['AQI Index']
model_null = sm.OLS(y, X).fit()


# 1. McLeod-Li Test for Nonlinearity (ARCH Test)
def mcleod_li_test(model_null):
    residuals = model_null.resid
    arch_test = het_arch(residuals)
    p_value = arch_test[1]
    print(f"McLeod-Li (ARCH) Test p-value: {p_value}")
    if p_value < 0.05:
        print("The test suggests potential nonlinearity (heteroscedasticity).")
    else:
        print("No significant evidence of nonlinearity (no heteroscedasticity).")

# 2. Kenan's RESET Test for Nonlinearity
def kenans_reset_test(model_null):
    reset_test = linear_reset(model_null, power=3, test_type='fitted')
    p_value = reset_test.pvalue
    print(f"\nKenan's RESET Test p-value: {p_value}")
    if p_value < 0.05:
        print("The test suggests potential nonlinearity in the model.")
    else:
        print("No significant evidence of nonlinearity in the model.")

# 3. Tsay's Test for Nonlinearity (ADF test on residuals)
def tsays_test(model_null):
    def tsay_test_func(residuals):
        adf_stat, p_value, _, _, critical_values, _ = adfuller(residuals)
        return p_value
    residuals_null = model_null.resid
    p_value_tsay = tsay_test_func(residuals_null)
    print(f"\nTsay Test p-value (ADF on residuals): {p_value_tsay}")
    if p_value_tsay < 0.05:
        print("The test suggests nonlinearity in the residuals (structural break or nonstationarity).")
    else:
        print("No significant evidence of nonlinearity in the residuals.")

# 4. Likelihood Ratio Test for Threshold Nonlinearity
def likelihood_ratio_test(model_null):
    data['Threshold'] = (data['Time'] > 50).astype(int)  # Define a threshold
    X_complex = sm.add_constant(data[['Time', 'Threshold']])
    model_complex = sm.OLS(y, X_complex).fit()
    
    log_likelihood_null = model_null.llf  # Log-likelihood of null model
    log_likelihood_complex = model_complex.llf  # Log-likelihood of alternative model
    
    # Likelihood Ratio Test
    lrt_statistic = -2 * (log_likelihood_null - log_likelihood_complex)
    df = len(model_complex.params) - len(model_null.params)
    p_value_lrt = chi2.sf(lrt_statistic, df)
    
    print(f"\nLikelihood Ratio Test p-value: {p_value_lrt}")
    if p_value_lrt < 0.05:
        print("The test suggests threshold nonlinearity (better fit with threshold model).")
    else:
        print("No significant evidence of threshold nonlinearity.")

# 5. Teresvitas' Neural Network Test for Nonlinearity
def teresvitas_nn_test(data):
    # Standardize the data before feeding it to the neural network
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data[['Time']])
    y_scaled = data['AQI Index'].values

    # Define the model using Input layer
    nn_model = Sequential([
        Input(shape=(1,)),  # Specify the input shape using Input layer
        Dense(10, activation='relu'),
        Dense(1)
    ])
    nn_model.compile(optimizer='adam', loss='mean_squared_error')

    # Train the neural network
    nn_model.fit(X_scaled, y_scaled, epochs=100, verbose=0)

    # Get predictions from the neural network
    nn_predictions = nn_model.predict(X_scaled)

    # Compare with the linear model predictions
    linear_predictions = model_null.predict(X)

    # Calculate the Mean Squared Error (MSE) of both models
    mse_nn = ((nn_predictions - y_scaled) ** 2).mean()
    mse_linear = ((linear_predictions - y_scaled) ** 2).mean()

    print("\nTeresvitas' Neural Network Test:")
    print(f"Neural Network MSE: {mse_nn}, Linear Model MSE: {mse_linear}")

    if mse_nn < mse_linear:
        print("The neural network model performs better, suggesting nonlinearity.")
    else:
        print("The linear model performs better, suggesting linearity.")

# 6. White's Neural Network Test for Nonlinearity
def whites_nn_test(data, model_null):
    # Standardize the data before feeding it to the neural network
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data[['Time']])
    y_scaled = data['AQI Index'].values

    # Define a neural network model
    nn_model = Sequential([
        Input(shape=(1,)),  # Specify the input shape using Input layer
        Dense(10, activation='relu'),
        Dense(1)
    ])
    nn_model.compile(optimizer='adam', loss='mean_squared_error')

    # Train the neural network
    nn_model.fit(X_scaled, y_scaled, epochs=100, verbose=0)

    # Get predictions from the neural network
    nn_predictions = nn_model.predict(X_scaled)

    # Calculate residuals from the linear model
    linear_predictions = model_null.predict(X)
    residuals_linear = y_scaled - linear_predictions

    # Calculate the MSE of the residuals from the neural network
    mse_nn = ((nn_predictions - y_scaled) ** 2).mean()

    print("\nWhite's Neural Network Test:")
    print(f"Neural Network MSE on residuals: {mse_nn}")

    if mse_nn < ((residuals_linear ** 2).mean()):
        print("The neural network better fits the residuals, suggesting nonlinearity.")
    else:
        print("The linear model residuals are better, suggesting linearity.")

# Run all tests
mcleod_li_test(model_null)
kenans_reset_test(model_null)
tsays_test(model_null)
likelihood_ratio_test(model_null)
teresvitas_nn_test(data)
whites_nn_test(data, model_null)

# **Interpretation of Nonlinearity & Heteroscedasticity Tests**

## **➤ McLeod-Li (ARCH) Test**
**p-value: 3.019 × 10⁻²⁶²** (Extremely low)  
📌 **Interpretation:**  
- Strong evidence of **heteroscedasticity** (variance changes over time).  
- The model may need **GARCH-type adjustments** to handle volatility.  

---

## **➤ Kenan's RESET Test**
**p-value: 1.202 × 10⁻⁴¹** (Extremely low)  
📌 **Interpretation:**  
- Suggests **model misspecification** due to nonlinearity.  
- Consider **higher-order polynomial terms** or **nonlinear transformations**.  

---

## **➤ Tsay Test (ADF on Residuals)**
**p-value: 0.0009** (Very low)  
📌 **Interpretation:**  
- Indicates **nonlinearity in residuals**, possibly due to **structural breaks** or **nonstationarity**.  
- Differencing or **regime-switching models** may help improve performance.  

---

## **➤ Likelihood Ratio Test**
**p-value: 0.3507** (Not significant)  
📌 **Interpretation:**  
- No evidence of **threshold nonlinearity**.  
- The model does not exhibit significant **regime shifts**.  

---

# **Neural Network-Based Linearity Tests**

## **➤ Teresvitas' Neural Network Test**
- **Neural Network MSE:** **11325.17**  
- **Linear Model MSE:** **10987.98**  
📌 **Interpretation:**  
- The linear model **performs slightly better**, suggesting that a linear approach may be sufficient.  
- Nonlinear models may not provide significant improvements.  

---

## **➤ White's Neural Network Test**
- **Neural Network MSE on residuals:** **11224.51**  
📌 **Interpretation:**  
- Residuals are better explained by a **linear model**.  
- No strong evidence that nonlinearity in residuals significantly affects predictions. 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler

# Load the data
data = pd.read_excel('AQI.xlsx')

# Assuming 'AQI Index' is the target variable
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)
ts = data['AQI Index']

# Split the data into training and testing sets
train_size = int(len(ts) * 0.8)
train, test = ts[0:train_size], ts[train_size:]

train = train.asfreq('D')
test = test.asfreq('D')


# ARIMA Model
def arima_model(train, test):
    model = ARIMA(train, order=(5,1,0))
    model_fit = model.fit()
    predictions = model_fit.forecast(steps=len(test))
    return predictions

# ARNN Model
def arnn_model(train, test):
    scaler = MinMaxScaler()
    train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))
    test_scaled = scaler.transform(test.values.reshape(-1, 1))
    
    # Prepare data for ARNN
    def create_dataset(data, look_back=1):
        X, Y = [], []
        for i in range(len(data) - look_back):
            X.append(data[i:(i+look_back), 0])
            Y.append(data[i + look_back, 0])
        return np.array(X), np.array(Y)
    
    look_back = 1
    trainX, trainY = create_dataset(train_scaled, look_back)
    testX, testY = create_dataset(test_scaled, look_back)
    
    model = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)
    model.fit(trainX, trainY)
    
    predictions = model.predict(testX)
    predictions = scaler.inverse_transform(predictions.reshape(-1, 1))
    
    # Adjust test size to match predictions
    test_adjusted = test.iloc[look_back:]
    
    return test_adjusted, predictions.flatten()

# ARIMA-ARNN Hybrid Model
def arima_arnn_hybrid(train, test):
    arima_predictions = arima_model(train, test)
    
    test_adjusted, arnn_predictions = arnn_model(train, test)
    
    # Combine predictions (simple average)
    hybrid_predictions = (arima_predictions[:len(arnn_predictions)] + arnn_predictions) / 2
    return test_adjusted, hybrid_predictions

# Evaluate models
def evaluate_model(test, predictions, num_features=1):
    mse = mean_squared_error(test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(test, predictions)
    mape = np.mean(np.abs((test - predictions) / test)) * 100
    r2 = r2_score(test, predictions)
    
    # Adjusted R² Calculation
    n = len(test)  # Number of samples
    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - num_features - 1))
    
    return rmse, mae, mape, r2, adj_r2

# Fit and evaluate ARIMA
arima_predictions = arima_model(train, test)
arima_rmse, arima_mae, arima_mape, arima_r2, arima_adj_r2 = evaluate_model(test, arima_predictions)

# Fit and evaluate ARNN
test_adjusted, arnn_predictions = arnn_model(train, test)
arnn_rmse, arnn_mae, arnn_mape, arnn_r2, arnn_adj_r2 = evaluate_model(test_adjusted, arnn_predictions)

# Fit and evaluate ARIMA-ARNN Hybrid
test_adjusted, hybrid_predictions = arima_arnn_hybrid(train, test)
hybrid_rmse, hybrid_mae, hybrid_mape, hybrid_r2, hybrid_adj_r2 = evaluate_model(test_adjusted, hybrid_predictions)

# Plot ARIMA results
plt.figure(figsize=(10, 6))
plt.plot(test.index, test, label='Actual', color='black')
plt.plot(test.index, arima_predictions, label='ARIMA Predictions', linestyle='dashed', color='blue')
plt.legend()
plt.xlabel("Date")
plt.ylabel("AQI Index")
plt.title("AQI Forecasting using ARIMA Model")
plt.show()

# Plot ARNN results
plt.figure(figsize=(10, 6))
plt.plot(test.index, test, label='Actual', color='black')
plt.plot(test_adjusted.index, arnn_predictions, label='ARNN Predictions', linestyle='dashed', color='red')
plt.legend()
plt.xlabel("Date")
plt.ylabel("AQI Index")
plt.title("AQI Forecasting using ARNN Model")
plt.show()

# Plot ARIMA-ARNN Hybrid results
plt.figure(figsize=(10, 6))
plt.plot(test.index, test, label='Actual', color='black')
plt.plot(test_adjusted.index, hybrid_predictions, label='ARIMA-ARNN Hybrid Predictions', linestyle='dashed', color='green')
plt.legend()
plt.xlabel("Date")
plt.ylabel("AQI Index")
plt.title("AQI Forecasting using ARIMA-ARNN Hybrid Model")
plt.show()

# Function to print evaluation results neatly
def print_evaluation_results(model_name, rmse, mae, mape, r2, adj_r2):
    print(f"{'-'*60}")
    print(f"{model_name} Model Evaluation Metrics")
    print(f"{'-'*60}")
    print(f"{'Metric':<20}{'Value'}")
    print(f"{'-'*60}")
    print(f"{'RMSE':<20}{rmse:.2f}")
    print(f"{'MAE':<20}{mae:.2f}")
    print(f"{'MAPE (%)':<20}{mape:.2f}")
    print(f"{'R²':<20}{r2:.4f}")
    print(f"{'Adjusted R²':<20}{adj_r2:.4f}")
    print(f"{'-'*60}\n")

# Print evaluation results for each model
print_evaluation_results("ARIMA", arima_rmse, arima_mae, arima_mape, arima_r2, arima_adj_r2)
print_evaluation_results("ARNN", arnn_rmse, arnn_mae, arnn_mape, arnn_r2, arnn_adj_r2)
print_evaluation_results("ARIMA-ARNN Hybrid", hybrid_rmse, hybrid_mae, hybrid_mape, hybrid_r2, hybrid_adj_r2)

# **Interpretations**

## **➤ ARIMA Model Performance**
> **🔹 Key Observations:**
> - **High RMSE (123.83) & MAE (109.88)** → Large errors in predictions.
> - **Very high MAPE (92.93%)** → On average, predictions deviate by ~93% from actual values.
> - **Negative R² (-0.4911)** → The model performs worse than simply predicting the mean of the data.
> - **Adjusted R² (-0.4939)** confirms poor performance.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;ARIMA alone does not capture AQI trends effectively.  
&nbsp;&nbsp;&nbsp;&nbsp;It struggles to predict variations in AQI, possibly due to nonlinear patterns in the data.

---

## **➤ ARNN (Artificial Neural Network) Model Performance**
> **🔹 Key Observations:**
> - **Lower RMSE (41.48) & MAE (30.54)** → Significantly reduced errors compared to ARIMA.
> - **Lower MAPE (19.91%)** → Predictions are much closer to actual values.
> - **High R² (0.8330)** → The model explains **83.3%** of the variance in the data.
> - **Adjusted R² (0.8326)** → Minimal overfitting.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;ARNN performs significantly better than ARIMA.  
&nbsp;&nbsp;&nbsp;&nbsp;It captures nonlinear dependencies better, leading to improved predictions.  
&nbsp;&nbsp;&nbsp;&nbsp;However, ARNN models require careful tuning (**e.g., number of neurons, epochs**) to avoid overfitting.

---

## **➤ ARIMA-ARNN Hybrid Model Performance**
> **🔹 Key Observations:**
> - **Moderate RMSE (74.05) & MAE (64.60)** → Errors are reduced compared to ARIMA but higher than ARNN.
> - **MAPE (51.29%)** → Improvement over ARIMA but worse than ARNN.
> - **Moderate R² (0.4676)** → Explains **46.76%** of the variance (**better than ARIMA but weaker than ARNN**).
> - **Adjusted R² (0.4665)** supports this conclusion.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;The hybrid model improves ARIMA but underperforms compared to ARNN alone.  
&nbsp;&nbsp;&nbsp;&nbsp;ARIMA contributes to linear patterns, while ARNN captures nonlinearities.  
&nbsp;&nbsp;&nbsp;&nbsp;The combination provides a middle-ground solution but still doesn’t outperform a pure ARNN.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Flatten, Input
from tensorflow.keras.optimizers import Adam
from tcn import TCN  # Temporal Convolutional Network

# Load AQI Data
data = pd.read_excel('AQI.xlsx')

# Convert Date to datetime and set as index
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)
data = data.asfreq('D')  # Ensure daily frequency

# Normalize AQI Values
scaler = MinMaxScaler(feature_range=(0, 1))
data['AQI Index'] = scaler.fit_transform(data[['AQI Index']])

# Split data into train & test sets
train_size = int(len(data) * 0.8)
train, test = data.iloc[:train_size], data.iloc[train_size:]

# Function to create input-output sequences
def create_sequences(data, look_back=5):
    X, Y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i + look_back])
        Y.append(data[i + look_back])
    return np.array(X), np.array(Y)

look_back = 5
X_train, Y_train = create_sequences(train['AQI Index'].values, look_back)
X_test, Y_test = create_sequences(test['AQI Index'].values, look_back)

# Reshape for LSTM, GRU, and TCN (samples, timesteps, features)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

# Function to build & train models with evaluation metrics
def train_model(model, X_train, Y_train, X_test, Y_test, model_name):
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    model.fit(X_train, Y_train, epochs=20, batch_size=16, verbose=1, validation_data=(X_test, Y_test))
    
    predictions = model.predict(X_test)
    predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
    Y_test_actual = scaler.inverse_transform(Y_test.reshape(-1, 1)).flatten()

    # Calculate evaluation metrics
    rmse = np.sqrt(mean_squared_error(Y_test_actual, predictions))
    mae = mean_absolute_error(Y_test_actual, predictions)
    mape = mean_absolute_percentage_error(Y_test_actual, predictions)
    r2 = r2_score(Y_test_actual, predictions)
    n = len(Y_test_actual)
    p = X_train.shape[1]  # Number of predictors
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    # Print evaluation metrics
    print(f'{model_name} Model Evaluation Metrics')
    print('-' * 50)
    print(f'Metric{" " * 15}Value')
    print('-' * 50)
    print(f'RMSE{" " * 11}{rmse:.2f}')
    print(f'MAE{" " * 14}{mae:.2f}')
    print(f'MAPE (%){" " * 6}{mape:.2f}')
    print(f'R²{" " * 16}{r2:.4f}')
    print(f'Adjusted R²{" " * 5}{adj_r2:.4f}')
    print('-' * 50)
    
    # Plot results
    plt.figure(figsize=(10, 5))
    plt.plot(test.index[look_back:], Y_test_actual, label='Actual AQI')
    plt.plot(test.index[look_back:], predictions, label=f'{model_name} Predictions')
    plt.legend()
    plt.title(f'{model_name} AQI Forecast')
    plt.show()

# 📌 **1️⃣ LSTM Model**
lstm_model = Sequential([
    Input(shape=(look_back, 1)),  # Explicit Input Layer
    LSTM(50, activation='relu', return_sequences=True),
    LSTM(50, activation='relu'),
    Dense(1)
])
train_model(lstm_model, X_train, Y_train, X_test, Y_test, 'LSTM')

# 📌 **2️⃣ GRU Model**
gru_model = Sequential([
    Input(shape=(look_back, 1)),  # Explicit Input Layer
    GRU(50, activation='relu', return_sequences=True),
    GRU(50, activation='relu'),
    Dense(1)
])
train_model(gru_model, X_train, Y_train, X_test, Y_test, 'GRU')

# 📌 **3️⃣ TCN Model**
tcn_model = Sequential([
    Input(shape=(look_back, 1)),  # Explicit Input Layer
    TCN(nb_filters=64, kernel_size=2, dilations=[1, 2, 4], return_sequences=True),
    Flatten(),
    Dense(1)
])
train_model(tcn_model, X_train, Y_train, X_test, Y_test, 'TCN')

# 📌 **4️⃣ NARX Model (Using LSTM)**
narx_model = Sequential([
    Input(shape=(look_back, 1)),  # Explicit Input Layer
    LSTM(50, activation='relu', return_sequences=True),
    LSTM(50, activation='relu'),
    Dense(1)
])
train_model(narx_model, X_train_exo, Y_train_exo, X_test_exo, Y_test_exo, 'NARX-LSTM')

# **Interpretations**

## ➤ LSTM Model Performance
> **🔹 Key Observations:**
> - **RMSE (41.33)** → Relatively low error values.
> - **MAE (30.18)** → Reasonable error.
> - **MAPE (0.20%)** → Very small deviation from actual values.
> - **R² (0.8346)** → High variance explained.
> - **Adjusted R² (0.8329)** → Minimal overfitting.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;LSTM performs strongly with minimal error and high explanatory power. It is a solid choice for time-series forecasting with high accuracy and reliability.

---

## ➤ GRU Model Performance
> **🔹 Key Observations:**
> - **RMSE (40.66)** → Slightly lower error values compared to LSTM.
> - **MAE (29.89)** → Slightly lower error than LSTM.
> - **MAPE (0.19%)** → Even smaller deviation from actual values than LSTM.
> - **R² (0.8399)** → Slightly higher variance explained than LSTM.
> - **Adjusted R² (0.8383)** → Slightly better fit with minimal overfitting.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;GRU performs similarly to LSTM, with slightly better error rates and predictive power, making it a strong alternative for time-series forecasting.

---

## ➤ TCN Model Performance
> **🔹 Key Observations:**
> - **RMSE (51.00)** → Higher error values compared to LSTM and GRU.
> - **MAE (38.77)** → Higher error compared to LSTM and GRU.
> - **MAPE (0.26%)** → Moderate deviation from actual values.
> - **R² (0.7481)** → Moderate variance explained, lower than both LSTM and GRU.
> - **Adjusted R² (0.7457)** → Indicates some overfitting.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;TCN underperforms compared to LSTM and GRU, likely due to its inability to capture the data's more complex dependencies effectively.

---

## ➤ NARX-LSTM Model Performance
> **🔹 Key Observations:**
> - **RMSE (42.02)** → Relatively low error values, but higher than LSTM and GRU.
> - **MAE (31.91)** → Slightly higher error than LSTM and GRU.
> - **MAPE (0.22%)** → Very small deviation from actual values, similar to LSTM and GRU.
> - **R² (0.8289)** → High variance explained, but slightly lower than LSTM and GRU.
> - **Adjusted R² (0.8272)** → Minimal overfitting.

📌 **Conclusion:**  
&nbsp;&nbsp;&nbsp;&nbsp;NARX-LSTM provides performance on par with LSTM and GRU, though slightly less optimal, particularly in terms of predictive power and error rates.
